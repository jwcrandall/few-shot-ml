{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FewShot Training - Practical 4\n",
    "___\n",
    "In this notebook we have an example of taking a pretained model and training it on a significaly different dataset\n",
    "\n",
    "## Part 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nshot\n",
    "import utils\n",
    "import csv\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "dir_head = pathlib.Path('/home/shared/')\n",
    "\n",
    "data_dir_head = dir_head / 'describable_textures'\n",
    "weights_file = dir_head / 'weights/rcnet_imagenet.pt'\n",
    "all_data_csv = \"/home/shared/practical_4_splits/all.csv\"\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Spliting a new data set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a dataset that contains images with describible textures. \n",
    "\n",
    "Given csv that contains the entire dataset look at what is in it and then split it up\n",
    "\n",
    "Validate that csvs were created correctly get stats on that\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `class_stats` function takes a csv path that contains the relative file path to the data and the class the data point it belongs to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_stats(data_csv_path):\n",
    "    with open(data_csv_path, mode='r') as file:\n",
    "        data = list(csv.reader(file))\n",
    "        classes = {}\n",
    "        for row in data:\n",
    "            if row[1] in classes:\n",
    "                classes[row[1]] += 1\n",
    "            else:\n",
    "                classes[row[1]] = 0 \n",
    "                \n",
    "        num_classes = len(classes)\n",
    "        num_datapoints = len(data)\n",
    "        \n",
    "        print(\"Total Number of Classes:\", num_classes)\n",
    "        print(\"Total Number of Data Points:\", num_datapoints)\n",
    "        \n",
    "        spacer = 20\n",
    "        for clas in classes:\n",
    "            \n",
    "            class_message = f'Class: {clas}'\n",
    "            class_message = class_message + ' ' * (spacer - len(class_message))\n",
    "            \n",
    "            count_message = f'Count: {classes[clas]}'\n",
    "            count_message = count_message + ' ' * (spacer - len(count_message))\n",
    "            \n",
    "            perc_message = f'Percentage:{classes[clas] /  num_datapoints * 100 : .2f}'\n",
    "            perc_message = perc_message + ' ' * (spacer - len(perc_message))\n",
    "            \n",
    "            print(class_message + count_message + perc_message)\n",
    "            \n",
    "        return list(classes.keys()), data\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the stats for the file containing all of the classes looking at the classes and datapoints in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, data = class_stats(all_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split` the data into appropriate train, test and validation splits default. Split is 70,15,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(classes, data, train_precentage=.70, val_percentage=.15):\n",
    "    random.shuffle(classes)\n",
    "    \n",
    "    train_index = int((len(classes)*train_precentage)//1)\n",
    "    val_index = train_index + int((len(classes)*val_percentage)//1)\n",
    "    \n",
    "    train_classes = classes[:train_index]\n",
    "    val_classes = classes[train_index:val_index]\n",
    "\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    \n",
    "    for row in data:\n",
    "        if row[1] in train_classes:\n",
    "            train_data.append(row)\n",
    "        elif row[1] in val_classes:\n",
    "            val_data.append(row)\n",
    "        else:\n",
    "            test_data.append(row)\n",
    "\n",
    "    train_path = utils.write_csv(train_data,\"train\")\n",
    "    val_path = utils.write_csv(val_data,\"val\")\n",
    "    test_path = utils.write_csv(test_data,\"test\")\n",
    "    \n",
    "    \n",
    "    return train_path, val_path, test_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, val_path, test_path = split(classes, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the stats for the newly created csv files, ensure they all contain distinct classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "classes, data = class_stats(train_path)\n",
    "print(\"\\nVal \")\n",
    "classes, data = class_stats(val_path)\n",
    "print(\"\\nTest \")\n",
    "classes, data = class_stats(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Convert to Dataset objects\n",
    "\n",
    "Using nshots load_data function to create a data object that maitains the file paths for the data only loading it when queried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nshot.load_data(data_dir_head,csv_path = train_path,load_type = \"csv\")\n",
    "val_dataset = nshot.load_data(data_dir_head,csv_path = val_path,load_type = \"csv\", min_classes = 5)\n",
    "test_dataset = nshot.load_data(data_dir_head,csv_path = test_path,load_type = \"csv\", min_classes = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Take a peek and see what the data looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_from_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Loading Model Weights\n",
    "Model and encoder need to be created before the weights are loaded and they must be the same arcitecture that the weights were saved for.\n",
    "\n",
    "In this case we use the default structure for nshot RCnet and a Resnet50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nshot.resnet50(device='cpu')\n",
    "\n",
    "model = nshot.RCNet(encoder, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = nshot.evaluate(model, test_dataset, num_episodes = 5, distractors=True)\n",
    "\n",
    "print(\"Loss:%f.3 Accuracy:%f.3\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading weights that have been trained on Imagenet populates the encoder and relationanal model with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_weights(weights_file,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation code: \n",
    "Evaluate loaded model on new dataset\n",
    "\n",
    "Visulization of classes and predictions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = nshot.evaluate(model, test_dataset, num_episodes = 5, distractors=True)\n",
    "\n",
    "print(\"Loss:%f.3 Accuracy:%f.3\" % (loss, accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fine tuning code:\n",
    "\n",
    "Training code \n",
    "\n",
    "Re evaluating and visulizations for diffrent learning rates and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.load_checkpoint(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = nshot.evaluate(model, test_dataset,num_episodes=5, distractors=True)\n",
    "\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
