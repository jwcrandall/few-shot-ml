{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning - Practical 1\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to grab all the imports we will need for this practical. We won't need too much to start out with, just NShot and our workshop utilities package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Sample episode from CIFAR 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this practical we will be working with a small subset of the CIFAR 10 dataset. This dataset is comprised of 10 classes of 32 x 32 pixel images. Our few-shot task will be subsampled down from these 10 classes. The default configuration will sample 5 classes and random and 5 images from each of those classes to define our support set and then pull the rest of the CIFAR 10 sample as the query set.\n",
    "\n",
    "The command below builds and resets the `cifar_sample` and `episode` directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.reset_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we assemble an episode from the generated directories. To get more information on our utility functions remember that you can always run `help(function_name)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode, support_classes = utils.get_episode('episode', preprocess_images=False)\n",
    "utils.visualize(episode, query_rows=2, scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(utils.visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Instantiate a ProtoNets model from the utils package\n",
    "\n",
    "For this exercise we will be using a basic ProtoTypical Network architecture along with a ResNet 18 encoder. Recall that ProtoNets simply uses a Euclidean distance to measure similarity between query points and the average of support class vectors. Since this operation is entirely predefined the only trainable parameters exist within the encoder architecture itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = utils.ResNet()\n",
    "model = utils.ProtoNets(encoder)\n",
    "\n",
    "model = model.eval()\n",
    "model.load('/home/shared/weights/protonets_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the model componenets (mostly the ResNet componenets) by printing the model. Note that the last fully connected layer is used to train ResNet when classically trained but is ignored in the few-shot context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model) # The focus of this notebook is not the model architecture so this is commented out by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Running Predictions\n",
    "Our last step before plugging our episode into our model is to run a little preprocessing on the images themselves. Right now they are stored in our episode object as PIL images. We need to normalize their size and convert them over to tensors. To do that we can make use if a standard preprocesssing function stored at `utils.preprocess`. This is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode['support'] = [utils.preprocess(image) for image in episode['support']]\n",
    "episode['query'] = [utils.preprocess(image) for image in episode['query']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model and our episode we can put them together and actually get an output from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(episode)\n",
    "\n",
    "print(utils.softmax(logits, dim=1))\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gave us a lot of numbers! Lets see if we can piece through this and nail down exactly what images these numbers correspond to. First we can save our episode seperately before and after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_episode, support_classes = utils.get_episode('episode', preprocess_images=False)\n",
    "tensor_episode, support_classes = utils.get_episode('episode', preprocess_images=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(tensor_episode)\n",
    "    \n",
    "utils.visualize_predictions(\n",
    "    images_episode,\n",
    "    logits,\n",
    "    num_queries=10,\n",
    "    support_classes=support_classes,\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Getting intuition for model behavior\n",
    "The block of code will let us run our model with a support set defined in the `episode_dir` and a query set defined in the `query_dir`. Take some time and play around with how changing the support set can impact performance on the query set. Here's a few things to try:\n",
    "- Add new classes to the support set that don't appear the query set\n",
    "- Add new classes in the query set that don't appear in the support set\n",
    "- Decrease or increase the number of support classes\n",
    "- Grab images from domains entirely different from CIFAR10\n",
    "\n",
    "```python\n",
    "images_episode, support_classes = utils.get_episode(episode_dir, query_dir=query_dir, preprocess_images=False)\n",
    "tensor_episode, support_classes = utils.get_episode(episode_dir, query_dir=query_dir, preprocess_images=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(tensor_episode)\n",
    "\n",
    "utils.visualize_predictions(\n",
    "    images_episode,\n",
    "    logits,\n",
    "    num_queries=10,\n",
    "    support_classes=support_classes\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_dir = 'episode'\n",
    "query_dir = 'cifar_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_episode, support_classes = utils.get_episode(episode_dir, query_dir=query_dir, preprocess_images=False)\n",
    "tensor_episode, support_classes = utils.get_episode(episode_dir, query_dir=query_dir, preprocess_images=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(tensor_episode)\n",
    "\n",
    "utils.visualize_predictions(\n",
    "    images_episode,\n",
    "    logits,\n",
    "    num_queries=10,\n",
    "    support_classes=support_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
