{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Learning - Practical 2\n",
    "___\n",
    "In this notebook we will walk through the basics of training a few-shot model. We will define our own training code, take a look at the definition of one possible model architecture, and explore a few standard hyper-parameters. \n",
    "\n",
    "Since we are low on compute resources and want to be able to see _something_ train we will be using a toy problem that is basically solved at this point: OmniGlot. This is essentially few-shot's MNIST. Its a collection of handwritten characters from a number of different languages. The key distinction between it and MNIST is that we only have about 20 characters per class but we have about 1000 classes. This will allow us to sample many subproblems from the dataset in order to form our episodes. \n",
    "\n",
    "### Part 0: Imports\n",
    "We start with what will become our traditional imports; Numpy, PyTorch and our course utilities package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils # Special package prepared for this course\n",
    "import torch # Core auto-differentiation library \n",
    "import numpy # Standard Python linear algebra library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Models\n",
    "We will use a standard 4-layer CNN architecture for our encoder and a simplified RCNet architecture for our few-shot model. These are defined in the utilities package under the submodule `practical_2`. Once we have gone through the training code it might be interesting to copy them into this notebook and play around with the definitions and see how changing them changes the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = utils.practical_2.SimpleCNN(device='cpu')             # Note that for this exercise everything will run on CPU\n",
    "model = utils.practical_2.ARelationalNet(encoder, fc_dim=16, device='cpu') # device notes are included to show one paradigm for managing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Training loop definition\n",
    "In order to keep everything organized we will define two functions to help us in our experimental setup. The first is called `train` and executes a single training iteration on the OmniGlot training data. Commentary on individual components is included as comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=1e-5, num_episodes=10, optimizer=None):\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    # We will bypass some of the complexity of loading and preprocessing\n",
    "    # data by making use of a pre-built dataloader object for the OmniGlot\n",
    "    # dataset. Note that building these dataloaders and data parsing tools\n",
    "    # is often one of the most time consuming parts of a DL project and\n",
    "    # has enough complexity to justfy its own course.\n",
    "    dataloader = utils.practical_2.get_omniglot(train=True, num_episodes=num_episodes)\n",
    "\n",
    "    # As training loops are developed you will often be asked to try out \n",
    "    # new ideas. This is of course something we need to plan for and adapt\n",
    "    # to as the requirements and technologies available shift. Because of\n",
    "    # this its always a good idea to write your code in a modular way and\n",
    "    # be able to alter function behavior with changing default behavior. \n",
    "    # \n",
    "    # The below instantiation of our optimizer is a good example of this,\n",
    "    # by default our optimizer is Adam. This is a reasonable choice to\n",
    "    # start off with but we might find that its a poor choice by the end\n",
    "    # of the project. Instead of changing the line of code in our\n",
    "    # experiment scripts we want to write our scripts such that if we\n",
    "    # re-run them exactly as before we get the old behavior but if we run\n",
    "    # them with new keyword arguments then we get the desired adjusted\n",
    "    # behavior, in this case maybe a better optimizer.\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "    # Place the model into train mode to ensure layers behave properly.\n",
    "    model.train()\n",
    "\n",
    "    # Here we have the primary for loop for the training process. We will\n",
    "    # be utilizing our pre-built dataloader to rapidly sample episodes from\n",
    "    # the OmniGlot dataset and run them through the model. We will then\n",
    "    # compute a loss value to measure our performance and compute the \n",
    "    # model gradients via backpropagation. Our final step (hehe, step and\n",
    "    # gradient step) is to take a single optimization step using our \n",
    "    # predefined optimization algorithm. \n",
    "    #\n",
    "    # This set of steps is a very standard algorithm configuration and is\n",
    "    # at the heart of nearly all deep learning training routines.\n",
    "    for episode, labels in dataloader:\n",
    "        \n",
    "        #Transfer labels to the correct device\n",
    "        labels = labels.to(model.device)\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # This might be an interesting place to insert some print statements\n",
    "        # and take a look at the current form of the episode.\n",
    "        # -----------------------------------------------------------------\n",
    "\n",
    "        # Run the episode through the model\n",
    "        logits = model(episode)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predictions = logits.max(1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predictions == labels.long()).sum().item()\n",
    "        acc = correct / total\n",
    "\n",
    "        # Compute the gradients via backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model weights via the chosen optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "  \n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    acc = sum(acc_list) / len(acc_list)\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Evaluation loop definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=10, large_images=False):    \n",
    "    # First thing we do is place the model in eval mode. Note that this \n",
    "    # does not do anything to gradient computation, rather it adjusts\n",
    "    # several torch.nn.Modules so we don't carry information from one\n",
    "    # episode to the next. For a more complete description see this\n",
    "    # stack overflow post:\n",
    "    # https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval\n",
    "    model.eval()\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    dataloader = utils.practical_2.get_omniglot(\n",
    "        train=False,\n",
    "        num_episodes=num_episodes,\n",
    "        large_images=large_images\n",
    "    )\n",
    "\n",
    "    # Here we have a very similar for-loop to our training loop. The big \n",
    "    # difference here is the lack of the backpropagation and optimization\n",
    "    # steps. We also have the added torch.no_grad() context statement.\n",
    "    # This line ensures we do not perform extra computation since we have\n",
    "    # no intention of computing gradients later on. \n",
    "    for episode, labels in dataloader:\n",
    "        \n",
    "        # Transfer labels to the correct device\n",
    "        labels = labels.to(model.device)\n",
    "\n",
    "        # Run the episode through model without saving the computation graph\n",
    "        with torch.no_grad():\n",
    "            logits = model(episode)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predictions = logits.max(1)\n",
    "        total = labels.size(0)\n",
    "        correct = (predictions == labels.long()).sum().item()\n",
    "        acc = correct / total\n",
    "\n",
    "        # Save computed loss and accuracy\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    acc = sum(acc_list) / len(acc_list)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Executing this cell can take a very long time!\n",
    "\n",
    "num_train_episodes = 100\n",
    "num_val_episodes = 50\n",
    "lr=1e-5\n",
    "\n",
    "for iteration in range(10):\n",
    "    \n",
    "    loss, acc = train(model, lr=lr, num_episodes=num_train_episodes)\n",
    "    \n",
    "    print(f'Iteration {iteration}, training loss: {loss}, training acc: {acc}')\n",
    "    \n",
    "    loss, acc = evaluate(model, num_episodes=num_val_episodes)\n",
    "    \n",
    "    print(f'Iteration {iteration}, validation loss: {loss}, validation acc: {acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Evaluation on a previously trained model\n",
    "Since we have limited compute lets also load some weights that were previously trained another dataset. This is a very similar architecture but it was trained on a dataset very different from Omniglot. It was also trained across many GPUs and using a variety of best practices that we didn't implement in our example training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nshot\n",
    "\n",
    "model = nshot.load_from_file_path(\n",
    "    '/home/shared/weights/metadataset.nsm',\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "evaluate(model, num_episodes=25, large_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So without ever seeing omniglot before this model gets 73% accuracy on brand new character recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
